{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "```\n",
    "Dustin Michels\n",
    "November 2017\n",
    "```\n",
    "\n",
    "This module:\n",
    "\n",
    "* Loads metadata, GO data, and taxonomy data CSVs into dataframes\n",
    "* Cleans up and merges together the dataframes\n",
    "* Provides some functions for further processing\n",
    "\n",
    "It is primarily intended to be called from another notebook using the IPython magic, `%run`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define some useful vars\n",
    "\"\"\"\n",
    "\n",
    "# To help output vars in namespace at end of file\n",
    "start_vars = dir()\n",
    "\n",
    "# paths to other stuff\n",
    "data_path = '../../data/'\n",
    "go_data_path = data_path + 'go_downloads/'\n",
    "tax_data_path = data_path + 'taxonomy'\n",
    "img_path = '../imgs/'\n",
    "\n",
    "meta_headers = [\n",
    "    'depth_(m)', 'temp_(c)', 'chlorophyl_(mg_chl/m3)',\n",
    "    'nitrate_(µmol/l)', 'oxygen_(µmol/kg)', 'salinity_(psu)',\n",
    "    'lat', 'long']\n",
    "\n",
    "samples = [\n",
    "    'ERR599104','ERR599090','ERR599008','ERR598948',\n",
    "    'ERR598992','ERR598999','ERR598995','ERR598980',\n",
    "    'ERR599142','ERR599078','ERR599031']\n",
    "\n",
    "region_map = {\n",
    "    'Southern Ocean (near Antarctica)':'SO',\n",
    "    'South Pacific (near the Marquesas)':'SP',\n",
    "    'North Pacific':'NP',\n",
    "    'North Atlantic (off the coast of Portugal)':'NA',\n",
    "    'Arabian Sea':'AS'}\n",
    "\n",
    "zone_map = {\n",
    "    'deep chlorophyll maximum layer':'DCM',\n",
    "    'surface water layer':'SURF',\n",
    "    'mesopelagic zone':'MESO'}\n",
    "\n",
    "# To help output vars in namespace at end of file\n",
    "global_vars = [x for x in dir() if x not in start_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions for Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get / Clean Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_meta_df():\n",
    "    \n",
    "    # Get and clean project meta data\n",
    "    meta_df = pd.read_csv(data_path + \"project_metadata_functional.csv\")\n",
    "\n",
    "    # Make column names neater\n",
    "    meta_df.columns = meta_df.columns.str.strip()\n",
    "    meta_df.columns = meta_df.columns.str.lower()\n",
    "    meta_df.columns = meta_df.columns.str.replace(' ', '_')\n",
    "    meta_df.rename(columns={'sample_details':'zone'}, inplace=True)\n",
    "\n",
    "    # Split lat and long into seperate columns\n",
    "    meta_df['lat'] = meta_df['lat/long'].str.split(',', 1).str[0]\n",
    "    meta_df['long'] = meta_df['lat/long'].str.split(',', 1).str[1]\n",
    "\n",
    "    # Clean up 'region' and 'zone' columns\n",
    "    meta_df['region'] = meta_df['region'].str.strip()\n",
    "    meta_df['zone'] = meta_df['zone'].str.strip()\n",
    "\n",
    "    # Indicate categorical data\n",
    "    meta_df['region'] = meta_df['region'].astype('category')\n",
    "    meta_df['zone'] = meta_df['zone'].astype('category')\n",
    "    meta_df['run_id'] = meta_df['run_id'].astype('category')\n",
    "\n",
    "    # Drop a few categories\n",
    "    meta_df.drop(\n",
    "        ['downloaded','link_to_info', 'student', 'lat/long'],\n",
    "        axis=1, inplace=True)\n",
    "    \n",
    "    return meta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get / Clean GO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_df_helper(idx):\n",
    "    \"\"\"Helper function for parsing GO CSVs\"\"\"\n",
    "    \n",
    "    filenames = meta_df['filename']\n",
    "    names = ['go_id', 'name', 'namespace', 'read_count']\n",
    "\n",
    "    # Read GO csv (for given index)\n",
    "    filepath = f\"{go_data_path}{filenames[idx]}\"\n",
    "    df = pd.read_csv(\n",
    "        filepath, header=None, names=names)\n",
    "    \n",
    "    # Add run_id column\n",
    "    df.insert(0, 'run_id', meta_df['run_id'][idx])\n",
    "    \n",
    "    # Sort by read_count\n",
    "    df.sort_values('read_count', ascending=False, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Add read_percent column, based on read_count\n",
    "    read_sum = df['read_count'].sum()\n",
    "    df['read_percent'] = (df['read_count']/read_sum)\n",
    "    \n",
    "    # Drop some columns\n",
    "    df.drop(['go_id','read_count'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_GO_df():\n",
    "    \"\"\"Call helper function with all GO annotations,\n",
    "    concatenating resulting dataframes together\n",
    "    \"\"\"\n",
    "    \n",
    "    df = get_df_helper(0)\n",
    "    for i in range(1, len(meta_df)):\n",
    "        new_df = get_df_helper(i)\n",
    "        df = pd.concat([df, new_df])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Indicate categorical data\n",
    "    df['run_id'] = df['run_id'].astype('category')\n",
    "    df['namespace'] = df['namespace'].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get / Clean Taxonomy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tax_df():\n",
    "    \"\"\"Read tax_summary.csv and into pandas\n",
    "    and clean data into tiday format\"\"\"\n",
    "    \n",
    "    mapping_dict = {\n",
    "        '00_SrnOcean_DCM':'ERR599104',\n",
    "        '01_SrnOcean_SURF':'ERR599090',\n",
    "        '02_SrnOcean_MESO':'ERR599008',\n",
    "        '03_SPac_DCM':'ERR598948',\n",
    "        '04_SPac_SURF':'ERR598992',\n",
    "        '05_SPac_MESO':'ERR598999',\n",
    "        '06_NPac_DCM':'ERR598995',\n",
    "        '07_NPac_MESO':'ERR598980',\n",
    "        '08_NPac_SURF':'ERR599142',\n",
    "        '09_NAtl_SURF':'ERR599078',\n",
    "        '10_AraSea_MESO':'ERR599031'\n",
    "    }\n",
    "    samples = list(mapping_dict.keys())\n",
    "    run_id = list(mapping_dict.values())\n",
    "\n",
    "    tax_df = pd.read_csv(f\"{tax_data_path}/tax_summary.csv\")\n",
    "    tax_df = tax_df[tax_df['taxlevel']==2]\n",
    "\n",
    "    tax_df.drop(\n",
    "        ['taxlevel', 'daughterlevels', 'rankID','total'],\n",
    "        axis=1, inplace=True)\n",
    "    \n",
    "    # Convert counts to percents\n",
    "    for samp in samples:\n",
    "        tax_df[samp] = tax_df[samp] / tax_df[samp].sum()\n",
    "\n",
    "    # Fix names\n",
    "    cols = ['name'] + run_id\n",
    "    tax_df.columns = cols\n",
    "    \n",
    "    # Restructure\n",
    "    tax_df = tax_df.melt(\n",
    "        id_vars='name', var_name='run_id',\n",
    "        value_name='tax_percent')\n",
    "    \n",
    "    # Rearrange columns\n",
    "    tax_df = tax_df[['name', 'run_id', 'tax_percent']]\n",
    "        \n",
    "    return tax_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def merge_data(meta_df, data_df):\n",
    "    \"\"\"Merge data_df (either GO or tax)\n",
    "    and meta_df together, using run_id\"\"\"\n",
    "    \n",
    "    full_df = data_df.merge(meta_df, on='run_id')\n",
    "    full_df.drop('filename', axis=1, inplace=True)\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DF with Top-N Most Abundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_n(full_df, by_col='read_percent', n=25):\n",
    "    \"\"\"Get superset of n most abundant rows from full_df\n",
    "    shared by all samples.\n",
    "    \n",
    "    Eg., 25 most abundant groups in GO_df (by 'read_percent')\n",
    "    for each run_id\"\"\"\n",
    "    \n",
    "    # Grab 25 largest, by taxonomy percent, for each group\n",
    "    s = full_df.groupby('run_id')[by_col].nlargest(n)\n",
    "    \n",
    "    # Query full dataframe for matching indicies\n",
    "    top_df = full_df.iloc[s.index.droplevel(0)]\n",
    "    \n",
    "    # To get superset, query full_df again with those names\n",
    "    names = top_df['name'].unique()\n",
    "    df = full_df[full_df['name'].isin(names)]\n",
    "    \n",
    "    # Make a deep copy (instead of using a slice of full_df)\n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename_shorter(df):\n",
    "    \"\"\"Rename region, zone, and run_id to shorter names,\n",
    "    based on maps at top of file\"\"\"\n",
    "    \n",
    "    df['region'] = df['region'].map(region_map)\n",
    "    df['zone'] = df['zone'].map(zone_map)\n",
    "    df['run_id'] = df['region'] + \" (\" + df['zone'] + \")\"\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def truncate_names(df, col=\"name\", n=35):\n",
    "    \"\"\"Truncate strings in give col at n characters\"\"\"\n",
    "    \n",
    "    df[col] = df[col].apply(\n",
    "        lambda x: (x[:35] + '...') if len(x) > 38 else x)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To help output vars in namespace at end of file\n",
    "start_vars = dir()\n",
    "\n",
    "meta_df = get_meta_df()\n",
    "go_df = get_GO_df()\n",
    "tax_df = get_tax_df()\n",
    "\n",
    "full_go = merge_data(meta_df, go_df)\n",
    "full_tax = merge_data(meta_df, tax_df)\n",
    "\n",
    "# To help output vars in namespace at end of file\n",
    "df_vars = [x for x in dir() if x not in start_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Info\n",
    "\n",
    "Helperful when this notebook is '%run' by a different notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import types\n",
    "# def load_data_info():\n",
    "#     print(\"Added these variables to the namespace:\")\n",
    "#     print(global_vars)\n",
    "\n",
    "#     print(\"\\nAs well as these data frames:\")\n",
    "#     print(df_vars)\n",
    "\n",
    "#     print(\"\\nAnd these functions:\")\n",
    "#     print([f.__name__ for f in globals().values() if type(f) == types.FunctionType])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
